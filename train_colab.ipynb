{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5135c33",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Install the required libraries for training with LoRA on CUDA GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages (this takes ~2 minutes)\n",
    "!pip install -q transformers>=4.34.0 peft>=0.5.0 datasets>=2.14.0\n",
    "!pip install -q bitsandbytes>=0.41.0 accelerate>=0.23.0\n",
    "!pip install -q sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d3757",
   "metadata": {},
   "source": [
    "## Step 2: Verify GPU Access\n",
    "\n",
    "Make sure you have a GPU allocated. If this shows \"No GPU\", go to Runtime ‚Üí Change runtime type ‚Üí Select GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbad829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"‚úì GPU: {gpu_name}\")\n",
    "    print(f\"‚úì VRAM: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    if \"T4\" in gpu_name:\n",
    "        print(\"\\nüìù Note: T4 is the free tier GPU. Training will take ~45-60 minutes.\")\n",
    "    elif \"V100\" in gpu_name:\n",
    "        print(\"\\nüìù Note: V100 detected. Training will take ~20-30 minutes.\")\n",
    "    elif \"A100\" in gpu_name:\n",
    "        print(\"\\nüìù Note: A100 detected. Training will take ~15-20 minutes.\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected!\")\n",
    "    print(\"Go to: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35350c91",
   "metadata": {},
   "source": [
    "## Step 3: Authenticate with Hugging Face\n",
    "\n",
    "You need a Hugging Face token to download models and datasets.\n",
    "\n",
    "Get your token here: https://huggingface.co/settings/tokens (select \"Read\" access)\n",
    "\n",
    "Then click the üîë icon on the left sidebar and add it as a secret named `HF_TOKEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "# Get token from Colab secrets\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úì Authenticated with Hugging Face\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to get HF_TOKEN from secrets.\")\n",
    "    print(\"\\nPlease:\")\n",
    "    print(\"1. Get your token from https://huggingface.co/settings/tokens\")\n",
    "    print(\"2. Click the üîë icon on the left sidebar\")\n",
    "    print(\"3. Add a secret named 'HF_TOKEN' with your token\")\n",
    "    print(\"\\nOr manually login:\")\n",
    "    !huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f30a74",
   "metadata": {},
   "source": [
    "## Step 4: Load and Format Dataset\n",
    "\n",
    "Download the RISC-V instruction dataset and format it for training.\n",
    "\n",
    "Each example consists of:\n",
    "- **Input**: Natural language description of an operation\n",
    "- **Output**: RISC-V assembly instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3008c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "DATASET_NAME = \"davidpirkl/riscv-instruction-specification\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"\\nLoading dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"‚úì Loaded {len(dataset)} examples\")\n",
    "\n",
    "# Show a sample\n",
    "print(\"\\nüìã Sample example:\")\n",
    "sample = dataset[0]\n",
    "print(f\"Description: {sample['description']}\")\n",
    "print(f\"Instruction: {sample['instructions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a48965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset with chat template\n",
    "print(\"Formatting dataset...\")\n",
    "\n",
    "def format_example(example):\n",
    "    \"\"\"Convert to chat format.\"\"\"\n",
    "    user_content = f\"Write the RISC-V assembly instruction for the following operation:\\n{example['description']}\"\n",
    "    assistant_content = example['instructions']\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_example, remove_columns=dataset.column_names)\n",
    "\n",
    "# Split train/validation (90/10)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "valid_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"‚úì Training examples: {len(train_dataset)}\")\n",
    "print(f\"‚úì Validation examples: {len(valid_dataset)}\")\n",
    "\n",
    "# Show formatted example\n",
    "print(\"\\nüìã Formatted example (first 300 chars):\")\n",
    "print(train_dataset[0][\"text\"][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87582dd",
   "metadata": {},
   "source": [
    "## Step 5: Load Model with 4-bit Quantization\n",
    "\n",
    "Load Mistral-7B with 4-bit quantization (QLoRA) to fit in GPU memory.\n",
    "\n",
    "**What is quantization?**\n",
    "- Reduces model weights from 16-bit to 4-bit precision\n",
    "- Saves ~4x memory with minimal quality loss\n",
    "- Makes 7B models trainable on consumer GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "print(\"Configuring 4-bit quantization...\")\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading model (this takes ~2 minutes on first run)...\")\n",
    "print(\"The model is ~14GB and will be cached for future runs.\\n\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"‚úì Model loaded with 4-bit quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a237c1b",
   "metadata": {},
   "source": [
    "## Step 6: Configure LoRA Adapters\n",
    "\n",
    "Apply LoRA (Low-Rank Adaptation) to the model.\n",
    "\n",
    "**What is LoRA?**\n",
    "- Freezes the 7.2B base parameters (they don't change)\n",
    "- Adds small \"adapter\" matrices (only 21M parameters, 0.29% of model)\n",
    "- Only trains these adapters, making fine-tuning much faster and cheaper\n",
    "- At inference, adapters modify the base model's behavior on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(\"Preparing model for training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"Configuring LoRA...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                           # LoRA rank\n",
    "    lora_alpha=16,                  # Scaling factor\n",
    "    target_modules=[                # Which layers get adapters\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",  # Attention\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",  # Feed-forward\n",
    "    ],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Show parameter counts\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\n‚úì LoRA applied\")\n",
    "print(f\"  Trainable: {trainable:,} parameters ({100*trainable/total:.3f}%)\")\n",
    "print(f\"  Frozen: {total-trainable:,} parameters\")\n",
    "print(f\"  Total: {total:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e651f7",
   "metadata": {},
   "source": [
    "## Step 7: Tokenize Dataset\n",
    "\n",
    "Convert text to token IDs that the model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize and prepare for training.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "print(\"‚úì Tokenization complete\")\n",
    "\n",
    "# Data collator handles padding\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0b6b2",
   "metadata": {},
   "source": [
    "## Step 8: Train the Model\n",
    "\n",
    "Train for 600 steps (~2 epochs on this dataset).\n",
    "\n",
    "**Expected time:**\n",
    "- T4 (free tier): ~45-60 minutes\n",
    "- V100: ~20-30 minutes  \n",
    "- A100: ~15-20 minutes\n",
    "\n",
    "**What to watch:**\n",
    "- **Loss** should decrease from ~4.0 to ~1.0\n",
    "- Training will save checkpoints every 100 steps\n",
    "- Evaluation runs every 50 steps\n",
    "\n",
    "‚òï Grab a coffee while this runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "OUTPUT_DIR = \"./adapters_riscv\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training\n",
    "    max_steps=600,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,  # Effective batch = 2\n",
    "    \n",
    "    # Optimization  \n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    warmup_steps=0,\n",
    "    \n",
    "    # Precision\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    \n",
    "    # Logging & eval\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=6,\n",
    "    \n",
    "    # Misc\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "print(\"Progress will be shown below:\")\n",
    "print(\"- Step X/600: Current progress\")\n",
    "print(\"- Loss: Should decrease from ~4.0 to ~1.0\")\n",
    "print(\"- Checkpoints saved every 100 steps\\n\")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b005498d",
   "metadata": {},
   "source": [
    "## Step 9: Save the Fine-Tuned Adapters\n",
    "\n",
    "Save the LoRA adapters so you can use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "final_dir = os.path.join(OUTPUT_DIR, \"final\")\n",
    "\n",
    "print(\"Saving adapters...\")\n",
    "model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "print(f\"\\n‚úì Adapters saved to: {final_dir}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - adapter_model.safetensors (~80MB) - The LoRA weights\")\n",
    "print(\"  - adapter_config.json - Configuration\")\n",
    "print(\"  - tokenizer files\")\n",
    "\n",
    "# Show file sizes\n",
    "!ls -lh {final_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64561b",
   "metadata": {},
   "source": [
    "## Step 10: Test the Fine-Tuned Model\n",
    "\n",
    "Try out your model! It should now generate RISC-V assembly instructions.\n",
    "\n",
    "**Important:** Use placeholder register names (rs1, rs2, rd) not specific ones (t0, s1, a0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(\"Loading model for inference...\")\n",
    "\n",
    "# Merge adapters for faster inference\n",
    "model = model.merge_and_unload()\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úì Model ready for inference\")\n",
    "\n",
    "def generate(query, max_tokens=100):\n",
    "    \"\"\"Generate RISC-V instruction from natural language.\"\"\"\n",
    "    # Format query\n",
    "    full_query = f\"Write the RISC-V assembly instruction for the following operation:\\n{query}\"\n",
    "    messages = [{\"role\": \"user\", \"content\": full_query}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# Test examples\n",
    "test_queries = [\n",
    "    \"Adds the values in rs1 and rs2, stores the result in rd\",\n",
    "    \"Subtracts the value in rs2 from rs1, stores the result in rd\",\n",
    "    \"Loads a word from memory at address rs1 into rd\",\n",
    "    \"Multiplies the values in two registers (rs1, rs2) and stores the result in rd\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Fine-Tuned Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{i}. {query}\")\n",
    "    result = generate(query)\n",
    "    print(f\"   ‚Üí {result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcc8f3",
   "metadata": {},
   "source": [
    "## Step 11: Interactive Testing\n",
    "\n",
    "Try your own queries! Type natural language descriptions and get RISC-V instructions.\n",
    "\n",
    "Type 'quit' to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f17c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Interactive RISC-V Assembly Generator\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTip: Use placeholder names (rs1, rs2, rd, imm)\")\n",
    "print(\"Example: 'Branches to label if rs1 equals rs2'\\n\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nYour query (or 'quit'): \").strip()\n",
    "    \n",
    "    if query.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"\\nDone!\")\n",
    "        break\n",
    "    \n",
    "    if not query:\n",
    "        continue\n",
    "    \n",
    "    print(\"\\nGenerating...\")\n",
    "    result = generate(query)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(result)\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47288e8",
   "metadata": {},
   "source": [
    "## Step 12: Download Adapters to Your Computer\n",
    "\n",
    "Download the trained adapters to use them locally or share with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of the adapters\n",
    "!zip -r adapters_riscv.zip {final_dir}\n",
    "\n",
    "print(\"\\n‚úì Adapters packaged\")\n",
    "print(\"\\nDownload options:\")\n",
    "print(\"1. Click the üìÅ icon on the left sidebar\")\n",
    "print(\"2. Find 'adapters_riscv.zip' and download it\")\n",
    "print(\"\\nOr save to Google Drive:\")\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !cp -r {final_dir} /content/drive/MyDrive/\n",
    "    print(f\"\\n‚úì Adapters copied to Google Drive: MyDrive/{os.path.basename(final_dir)}\")\n",
    "except:\n",
    "    print(\"\\n(Mount Google Drive manually if you want to save there)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a278a",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully fine-tuned a 7B parameter language model!\n",
    "\n",
    "**What you learned:**\n",
    "- ‚úÖ Parameter-efficient fine-tuning with LoRA\n",
    "- ‚úÖ 4-bit quantization for memory optimization\n",
    "- ‚úÖ Complete training pipeline from data to inference\n",
    "- ‚úÖ How to adapt foundation models for specialized tasks\n",
    "\n",
    "**Next steps:**\n",
    "1. Try fine-tuning on your own dataset\n",
    "2. Experiment with different hyperparameters\n",
    "3. Share your adapters on Hugging Face Hub\n",
    "4. Deploy your model in an application\n",
    "\n",
    "**Resources:**\n",
    "- Full project: https://github.com/jschroeder-mips/llm_training_mlx\n",
    "- PEFT docs: https://huggingface.co/docs/peft\n",
    "- LoRA paper: https://arxiv.org/abs/2106.09685"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
