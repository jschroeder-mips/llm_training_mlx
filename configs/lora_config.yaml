
model: mistralai/Mistral-7B-Instruct-v0.3
train: true
data: .
batch_size: 2
iters: 600
learning_rate: 1e-5
steps_per_eval: 50
adapter_path: adapters.npz
save_every: 100
lora_parameters:
  rank: 16
  alpha: 16
  dropout: 0.0
  scale: 16.0
